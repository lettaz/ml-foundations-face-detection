{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11b4cc74",
   "metadata": {},
   "source": [
    "# Home Assignment No. 1: Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda88da5",
   "metadata": {},
   "source": [
    "In this part of the homework, you are to solve several theoretical problems related to machine learning algorithms.\n",
    "\n",
    "* For every separate problem you can get **INTERMEDIATE scores**.\n",
    "\n",
    "\n",
    "* Your solution must me **COMPLETE**, i.e. contain all required formulas/proofs/detailed explanations.\n",
    "\n",
    "\n",
    "* You must write your solution for each problem right after the words **YOUR SOLUTION**. Attaching pictures of your handwriting is allowed, but **highly discouraged**.\n",
    "\n",
    "## $\\LaTeX$ in Jupyter\n",
    "\n",
    "Jupyter has constantly improving $\\LaTeX$ support. Below are the basic methods to write **neat, tidy, and well typeset** equations in your notebooks:\n",
    "\n",
    "* to write an **inline** equation use \n",
    "```markdown\n",
    "$ your latex equation here $\n",
    "```\n",
    "\n",
    "* to write an equation, that is **displayed on a separate line** use \n",
    "```markdown\n",
    "$$ your latex equation here $$\n",
    "```\n",
    "\n",
    "* to write **cases of equations** use \n",
    "```markdown\n",
    "$$ left-hand-side = \\begin{cases}\n",
    "                     right-hand-side on line 1, & \\text{condition} \\\\\n",
    "                     right-hand-side on line 2, & \\text{condition} \\\\\n",
    "                    \\end{cases} $$\n",
    "```\n",
    "\n",
    "* to write a **block of equations** use \n",
    "```markdown\n",
    "$$ \\begin{align}\n",
    "    left-hand-side on line 1 &= right-hand-side on line 1 \\\\\n",
    "    left-hand-side on line 2 &= right-hand-side on line 2\n",
    "   \\end{align} $$\n",
    "```\n",
    "\n",
    "The **ampersand** (`&`) aligns the equations horizontally and the **double backslash**\n",
    "(`\\\\`) creates a new line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518781b8-6d90-494c-99dd-ddd5ffaa90a8",
   "metadata": {},
   "source": [
    "## Task 1. Locally Weighted Linear Regression [6 points]\n",
    "\n",
    "Under the assumption that $\\mathbf{X}^\\top W(\\mathbf{x}_0) \\mathbf{X}$ is inverible, derive the closed form solution for the LWR problem, defined in Task 3 of the practical part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d699613-ed8c-4304-a85f-b2b9a7735a47",
   "metadata": {},
   "source": [
    "### Your solution:\n",
    "\n",
    "In Locally Weighted Linear Regression, we want to predict the output $ \\hat{y}_0 $ for a new input $ \\mathbf{x}_0 $ based on a weighted linear regression model. The weights are determined by a kernel function that gives more importance to points close to $ \\mathbf{x}_0 $.\n",
    "\n",
    "#### Assumptions\n",
    "- Let $ \\mathbf{X} $ be the design matrix containing the training data points.\n",
    "- Let $ \\mathbf{y} $ be the vector of target values.\n",
    "- The weight matrix $ W(\\mathbf{x}_0) $ is defined as a diagonal matrix where each diagonal element $ w^{(i)}(\\mathbf{x}_0) $ is computed using a kernel function, typically a Gaussian kernel:\n",
    "  $$\n",
    "  w^{(i)}(\\mathbf{x}_0) = \\exp\\left(-\\frac{\\|\\mathbf{x}_0 - \\mathbf{x}_i\\|^2}{2\\tau^2}\\right)\n",
    "  $$\n",
    "  where $ \\tau $ is the bandwidth parameter.\n",
    "\n",
    "#### Closed-Form Solution Derivation\n",
    "The closed-form solution for LWR can be derived as follows:\n",
    "\n",
    "1. **Weight Matrix**:\n",
    "   Define the weight matrix $ W(\\mathbf{x}_0) $:\n",
    "   $$\n",
    "   W(\\mathbf{x}_0) = \\text{diag}(w^{(1)}(\\mathbf{x}_0), w^{(2)}(\\mathbf{x}_0), \\ldots, w^{(m)}(\\mathbf{x}_0))\n",
    "   $$\n",
    "\n",
    "2. **Linear Model**:\n",
    "   The linear model can be expressed as:\n",
    "   $$\n",
    "   \\hat{y}_0 = \\mathbf{x}_0^\\top \\theta\n",
    "   $$\n",
    "   where $ \\theta $ are the parameters we want to estimate.\n",
    "\n",
    "3. **Objective Function**:\n",
    "   The objective is to minimize the weighted sum of squared errors:\n",
    "   $$\n",
    "   J(\\theta) = \\sum_{i=1}^{m} w^{(i)}(\\mathbf{x}_0) (y_i - \\mathbf{x}_i^\\top \\theta)^2\n",
    "   $$\n",
    "\n",
    "4. **Normal Equations**:\n",
    "   The normal equations for the weighted least squares can be derived by setting the gradient of the objective function to zero:\n",
    "   $$\n",
    "   \\theta = (X^\\top W(\\mathbf{x}_0) X)^{-1} X^\\top W(\\mathbf{x}_0) \\mathbf{y}\n",
    "   $$\n",
    "\n",
    "5. **Final Closed-Form Solution**:\n",
    "   Thus, the closed-form solution for the predicted value $ \\hat{y}_0 $ at the point $ \\mathbf{x}_0 $ is:\n",
    "   $$\n",
    "   \\hat{y}_0 = \\mathbf{x}_0^\\top (X^\\top W(\\mathbf{x}_0) X)^{-1} X^\\top W(\\mathbf{x}_0) \\mathbf{y}\n",
    "   $$\n",
    "\n",
    "\n",
    "The closed-form solution for Locally Weighted Linear Regression allows us to compute the predicted output $ \\hat{y}_0 $ for a new input $ \\mathbf{x}_0 $ using the weighted least squares approach. The weights are determined by the proximity of the training points to $ \\mathbf{x}_0 $, allowing the model to adapt locally.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599a55ca",
   "metadata": {},
   "source": [
    "I'll help you derive the closed-form solution for Locally Weighted Linear Regression (LWR) in more detail.\n",
    "\n",
    "From the practical part (ML_2024_HW01_practice.ipynb), we can see the objective function is:\n",
    "```\n",
    "startLine: 394\n",
    "endLine: 396\n",
    "```\n",
    "\n",
    "Let's derive the solution step by step:\n",
    "\n",
    "### Derivation of Closed-Form Solution for LWR\n",
    "\n",
    "1. **Setup**:\n",
    "   - Let $\\mathbf{X} \\in \\mathbb{R}^{m \\times d}$ be the design matrix\n",
    "   - Let $\\mathbf{y} \\in \\mathbb{R}^m$ be the target vector\n",
    "   - Let $W(\\mathbf{x}_0)$ be the diagonal weight matrix where $W_{ii} = w^{(i)}(\\mathbf{x}_0)$\n",
    "\n",
    "2. **Objective Function**:\n",
    "   $$J(\\theta(\\mathbf{x}_0)) = \\sum_{i=1}^m w^{(i)}(\\mathbf{x}_0)(y_i - \\mathbf{x}_i^\\top\\theta(\\mathbf{x}_0))^2$$\n",
    "\n",
    "   In matrix form:\n",
    "   $$J(\\theta(\\mathbf{x}_0)) = (\\mathbf{y} - \\mathbf{X}\\theta(\\mathbf{x}_0))^\\top W(\\mathbf{x}_0)(\\mathbf{y} - \\mathbf{X}\\theta(\\mathbf{x}_0))$$\n",
    "\n",
    "3. **Find Minimum**:\n",
    "   Take the gradient with respect to $\\theta(\\mathbf{x}_0)$ and set to zero:\n",
    "   \n",
    "   $$\\begin{align}\n",
    "   \\frac{\\partial J}{\\partial \\theta(\\mathbf{x}_0)} &= -2\\mathbf{X}^\\top W(\\mathbf{x}_0)(\\mathbf{y} - \\mathbf{X}\\theta(\\mathbf{x}_0)) = 0\n",
    "   \\end{align}$$\n",
    "\n",
    "4. **Solve for $\\theta(\\mathbf{x}_0)$**:\n",
    "   $$\\begin{align}\n",
    "   \\mathbf{X}^\\top W(\\mathbf{x}_0)\\mathbf{y} &= \\mathbf{X}^\\top W(\\mathbf{x}_0)\\mathbf{X}\\theta(\\mathbf{x}_0) \\\\\n",
    "   \\theta(\\mathbf{x}_0) &= (\\mathbf{X}^\\top W(\\mathbf{x}_0)\\mathbf{X})^{-1}\\mathbf{X}^\\top W(\\mathbf{x}_0)\\mathbf{y}\n",
    "   \\end{align}$$\n",
    "\n",
    "5. **Prediction at $\\mathbf{x}_0$**:\n",
    "   The prediction at point $\\mathbf{x}_0$ is:\n",
    "   $$\\begin{align}\n",
    "   \\hat{y}_0 &= \\mathbf{x}_0^\\top\\theta(\\mathbf{x}_0) \\\\\n",
    "   &= \\mathbf{x}_0^\\top(\\mathbf{X}^\\top W(\\mathbf{x}_0)\\mathbf{X})^{-1}\\mathbf{X}^\\top W(\\mathbf{x}_0)\\mathbf{y}\n",
    "   \\end{align}$$\n",
    "\n",
    "### Verification of Solution\n",
    "\n",
    "1. **Uniqueness**: The solution is unique because $\\mathbf{X}^\\top W(\\mathbf{x}_0)\\mathbf{X}$ is assumed to be invertible.\n",
    "\n",
    "2. **Optimality**: The solution is optimal because:\n",
    "   - The objective function is convex (sum of weighted squared terms)\n",
    "   - We found a critical point where gradient is zero\n",
    "   - The Hessian $2\\mathbf{X}^\\top W(\\mathbf{x}_0)\\mathbf{X}$ is positive definite (since it's invertible and $W(\\mathbf{x}_0)$ is positive diagonal)\n",
    "\n",
    "Therefore, we have derived the closed-form solution for LWR:\n",
    "$$\\hat{y}_0 = \\mathbf{x}_0^\\top(\\mathbf{X}^\\top W(\\mathbf{x}_0)\\mathbf{X})^{-1}\\mathbf{X}^\\top W(\\mathbf{x}_0)\\mathbf{y}$$\n",
    "\n",
    "This matches the solution given in the practical part:\n",
    "```\n",
    "startLine: 400\n",
    "endLine: 402\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff2a8c7",
   "metadata": {},
   "source": [
    "## Task 2. Multiclass Naive Bayes Classifier [4 points]\n",
    "\n",
    "Let us consider **multiclass classification problem** with classes $C_1, \\dots, C_K$.\n",
    "\n",
    "Assume that all $d$ features $\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_d \\end{bmatrix}$ are **binary**, i.e. $x_{i} \\in \\{0, 1\\}$ for $i = \\overline{1, d}$ **or** feature vector $\\mathbf{x} \\in \\{0, 1\\}^d$.\n",
    "\n",
    "Show that the decision rule of a **Naive Bayes Classifier** can be represented as $\\arg\\max$ of linear functions of the input.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "**Hint**: use the **maximum a posteriori** (MAP) decision rule: $\\hat{y} = \\arg\\max\\limits_{y \\in \\overline{1, K}} p(y)p(\\mathbf{x}|y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3372e710",
   "metadata": {},
   "source": [
    "### Your solution:\n",
    "\n",
    "In a multiclass classification problem with classes $ C_1, C_2, \\ldots, C_K $, we want to derive the decision rule for a Naive Bayes Classifier based on the maximum a posteriori (MAP) decision rule.\n",
    "\n",
    "#### Assumptions\n",
    "- Let $ \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_d \\end{bmatrix} $ be the feature vector.\n",
    "- Assume that all $ d $ features are binary, i.e., $ x_i \\in \\{0, 1\\} $ for $ i = 1, 2, \\ldots, d $.\n",
    "\n",
    "#### Maximum A Posteriori (MAP) Decision Rule\n",
    "The MAP decision rule states that we choose the class $ \\hat{y} $ that maximizes the posterior probability:\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_{y \\in \\{C_1, C_2, \\ldots, C_K\\}} p(y | \\mathbf{x})\n",
    "$$\n",
    "\n",
    "Using Bayes' theorem, we can express the posterior probability as:\n",
    "$$\n",
    "p(y | \\mathbf{x}) = \\frac{p(\\mathbf{x} | y) p(y)}{p(\\mathbf{x})}\n",
    "$$\n",
    "\n",
    "Since $ p(\\mathbf{x}) $ is constant for all classes, we can simplify the decision rule to:\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_{y \\in \\{C_1, C_2, \\ldots, C_K\\}} p(\\mathbf{x} | y) p(y)\n",
    "$$\n",
    "\n",
    "#### Naive Bayes Assumption\n",
    "The Naive Bayes classifier assumes that the features are conditionally independent given the class label. Therefore, we can express the likelihood \\( p(\\mathbf{x} | y) \\) as:\n",
    "$$\n",
    "p(\\mathbf{x} | y) = \\prod_{i=1}^{d} p(x_i | y)\n",
    "$$\n",
    "\n",
    "Substituting this back into the decision rule gives:\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_{y \\in \\{C_1, C_2, \\ldots, C_K\\}} \\left( p(y) \\prod_{i=1}^{d} p(x_i | y) \\right)\n",
    "$$\n",
    "\n",
    "#### Logarithmic Transformation\n",
    "To simplify the computation, we can take the logarithm of the decision rule:\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_{y} \\left( \\log(p(y)) + \\sum_{i=1}^{d} \\log(p(x_i | y)) \\right)\n",
    "$$\n",
    "\n",
    "This shows that the decision rule can be expressed as a linear combination of the log-probabilities of the features given the class, plus the log-prior probability of the class.\n",
    "\n",
    "### Conclusion\n",
    "The decision rule of a Naive Bayes Classifier can be represented as:\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_{y} \\left( \\log(p(y)) + \\sum_{i=1}^{d} \\log(p(x_i | y)) \\right)\n",
    "$$\n",
    "\n",
    "This formulation demonstrates that the decision rule is indeed a linear function of the input features, where the coefficients are derived from the log-probabilities of the features given the class.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
